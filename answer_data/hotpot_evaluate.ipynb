{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "# ROUGE-L\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def update_answer(metrics, prediction, gold):\n",
    "    em = exact_match_score(prediction, gold)\n",
    "    f1, prec, recall = f1_score(prediction, gold)\n",
    "    metrics['em'] += float(em)\n",
    "    metrics['f1'] += f1\n",
    "    metrics['prec'] += prec\n",
    "    metrics['recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def eval(prediction_file, gold_file, means = 'rougeL'):\n",
    "    with open(prediction_file) as f:\n",
    "        prediction = json.load(f)\n",
    "    with open(gold_file) as f:\n",
    "        gold = json.load(f)\n",
    "        \n",
    "    scorer = rouge_scorer.RougeScorer([means], use_stemmer=True)\n",
    "    total_score_f1 = 0 \n",
    "    total_score_recall = 0\n",
    "    total_score_precision = 0\n",
    "    \n",
    "    metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0}\n",
    "    for dp in gold:\n",
    "        cur_id = dp['_id']\n",
    "        can_eval_joint = True\n",
    "        if cur_id not in prediction:\n",
    "            print('missing answer {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            em, prec, recall = update_answer(\n",
    "                metrics, prediction[cur_id], dp['answer'])\n",
    "            \n",
    "            scores  = scorer.score(dp['answer'], prediction[cur_id])  \n",
    "            total_score_f1 += scores[means].fmeasure\n",
    "            total_score_recall+= scores[means].recall\n",
    "            total_score_precision+= scores[means].precision\n",
    "            \n",
    "    N = len(gold)\n",
    "    print(N)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "\n",
    "    print(metrics)\n",
    "    print(f\"ROUGE-L F1 Score: {total_score_f1/N}\")\n",
    "    print(f\"ROUGE-L Recall: {total_score_recall/N}\")\n",
    "    print(f\"ROUGE-L Precision: {total_score_precision/N}\")\n",
    "    \n",
    "def evalf(prediction_path, gold_file, means = 'rougeL'):\n",
    "    files = os.listdir(prediction_path)\n",
    "    with open(gold_file) as f:\n",
    "        gold = json.load(f)\n",
    "\n",
    "    # means ['rouge1', 'rouge2', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer([means], use_stemmer=True)\n",
    "    total_score_f1 = 0 \n",
    "    total_score_recall = 0\n",
    "    total_score_precision = 0\n",
    "    \n",
    "    metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0,}\n",
    "    N = 0\n",
    "    for dp in gold:\n",
    "        cur_id = dp['_id']\n",
    "        can_eval_joint = True\n",
    "        if cur_id not in files:\n",
    "            # print('missing answer {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            with open(f'{prediction_path}/{cur_id}/output.txt', 'r', encoding='utf-8') as file:\n",
    "                outputs = file.read()\n",
    "            em, prec, recall = update_answer(\n",
    "                metrics, outputs, dp['answer'])\n",
    "            scores  = scorer.score(dp['answer'], outputs)  \n",
    "            total_score_f1 += scores[means].fmeasure\n",
    "            total_score_recall+= scores[means].recall\n",
    "            total_score_precision+= scores[means].precision\n",
    "            N += 1\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "\n",
    "    print(N)\n",
    "    print(metrics)\n",
    "    print(f\"ROUGE-L F1 Score: {total_score_f1/N}\")\n",
    "    print(f\"ROUGE-L Recall: {total_score_recall/N}\")\n",
    "    print(f\"ROUGE-L Precision: {total_score_precision/N}\")\n",
    "    \n",
    "\n",
    "def show_the_question(idx, prediction_path, query = None):\n",
    "\n",
    "    with open(\"../hotpot/hotpot_dev_distractor_v1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    questions_id = [entry[\"_id\"] for entry in dataset]\n",
    "    questions = [entry['question'] for entry in dataset]\n",
    "    tmp_context = [entry['context'] for entry in dataset]\n",
    "    answers = [entry['answer'] for entry in dataset]\n",
    "    true_context = [entry['supporting_facts'] for entry in dataset]\n",
    "\n",
    "    def find_context(lst, target):\n",
    "        sub = []\n",
    "        for sublist in lst:\n",
    "            if sublist[0] == target[0]:  # match str\n",
    "                sub = sublist[1]  # return sentence list  \n",
    "                break\n",
    "        if len(sub) <= target[1]:\n",
    "            return \"\"    \n",
    "        return sub[target[1]]  # return sentence\n",
    "\n",
    "    if query is not None:\n",
    "        for i,q in enumerate(questions):\n",
    "            if(query in q):\n",
    "                idx = i    \n",
    "    \n",
    "    useful_info = [find_context(tmp_context[idx], target) for target in true_context[idx]]\n",
    "    print(\"question: \", questions[idx])\n",
    "    print(\"answer: \", answers[idx])\n",
    "    print(\"useful info: \", useful_info)\n",
    "    \n",
    "    \n",
    "    with open(f'{prediction_path}/{questions_id[idx]}/output.txt', 'r', encoding='utf-8') as file:\n",
    "        output = file.read()\n",
    "    print(\"prediction: \", output)\n",
    "    \n",
    "    f1, prec, recall = f1_score(output, answers[idx])\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"prec: \", prec)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"exact match: \", exact_match_score(output, answers[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute the json output\n",
    "prediction_files = [\n",
    "    \"Answers_glm_RAG_div_hotpot_dev_distractor_v1_sonar.json\",\n",
    "    \"Answers_glm_RAG_div_hotpot_dev_distractor_v1_multi.json\"\n",
    "    ]\n",
    "gold_file = \"../hotpot/hotpot_dev_distractor_v1.json\"\n",
    "\n",
    "eval(prediction_files[1], gold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute the text output (in files)\n",
    "prediction_path =[\"hotpot_openai\",\"hotpot_bce\", \"hotpot_multi\",\"hotpot_sonar\"]\n",
    "gold_file = \"../hotpot/hotpot_dev_distractor_v1.json\"\n",
    "\n",
    "evalf(prediction_path[0], gold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the details of the idx-th question/answer\n",
    "idx = 0\n",
    "show_the_question(idx, prediction_path[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpc-glm-voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
